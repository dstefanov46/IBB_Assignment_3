{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BJSRHyso-8DJ"
   },
   "outputs": [],
   "source": [
    "# LIBRARIES\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_curve\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.layers.core import Dense, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import *\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2Z6lXkCqff9R"
   },
   "outputs": [],
   "source": [
    "# Function to get all the gender classes in our dataset and check the gender of every one of the 100 subjects\n",
    "def get_gender_classes(id_classes):\n",
    "    gender_classes = []\n",
    "    gender_dict = {}\n",
    "    for id_class in id_classes:\n",
    "        tmp_file_name = 'awe/' + id_class + '/annotations.json'\n",
    "        tmp_file = open(tmp_file_name, 'r')\n",
    "        file_lines = tmp_file.readlines()\n",
    "        for line in file_lines:\n",
    "            if ('\"gender\"') in line:\n",
    "                gender = line.split('\"gender\": ')[1].replace('\\n', '').replace('\"', '')\n",
    "                gender_dict[id_class] = gender\n",
    "                if gender not in gender_classes:\n",
    "                    gender_classes.append(gender)\n",
    "    return gender_classes, gender_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eGCTURORfg-B"
   },
   "outputs": [],
   "source": [
    "# Function that converts the ordinal number of some subject to the actual id_class of that subject in the dataset\n",
    "def int_to_class(integer):\n",
    "    tmp_class = str(integer)\n",
    "    for k in range(3 - len(tmp_class)):\n",
    "        tmp_class = '0' + tmp_class\n",
    "    return tmp_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PwQvlJbN7mUD"
   },
   "outputs": [],
   "source": [
    "def get_model_name(n):\n",
    "    return 'model_' + str(n) + '.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qs9P-do_AswN",
    "outputId": "8f719e66-f9ba-4b63-dc90-dab76ff91c0a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'awe',\n",
       " 'awe_dataset',\n",
       " 'ear_recognition.ipynb',\n",
       " 'models',\n",
       " 'report',\n",
       " 'test',\n",
       " 'train']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o7421aEwx5xW",
    "outputId": "05feec7c-7038-49b3-c6e3-4d755a1045d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "553467904/553467096 [==============================] - 8s 0us/step\n"
     ]
    }
   ],
   "source": [
    "vgg16_model = keras.applications.vgg16.VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cicX2jS4_qaf",
    "outputId": "0930b855-2089-493a-c448-2203ce7f6223"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg16_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fvmTeXPQAfod"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "for layer in vgg16_model.layers:\n",
    "    model.add(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-O60ZC3TBTl9",
    "outputId": "f9884386-e98e-4e88-b5b6-5c807d48d349"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7f7d77525320>"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._layers.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9YBDTNL7BgpU"
   },
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "gZO7TglkClan"
   },
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "XdmQ1KINDT48"
   },
   "outputs": [],
   "source": [
    "predictions_layer = model._layers[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5PKzaQDUGcHX"
   },
   "outputs": [],
   "source": [
    "model._layers.remove(predictions_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OmXmirhxGz34",
    "outputId": "9b26a1e3-3591-4ed5-a559-930aa7592dfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 2002      \n",
      "=================================================================\n",
      "Total params: 134,262,546\n",
      "Trainable params: 2,002\n",
      "Non-trainable params: 134,260,544\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "GSZryOSES3n_"
   },
   "outputs": [],
   "source": [
    "model.compile(Adam(lr=.0001), loss='categorical_crossentropy', metrics=['accuracy', keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "xqXr6W6cBC5S"
   },
   "outputs": [],
   "source": [
    "# Get the different classes by ID in the dataset\n",
    "id_classes = []\n",
    "for i in range(1, 101):\n",
    "    tmp_class = int_to_class(i)\n",
    "    id_classes.append(tmp_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "bYwq2m9sfnmO"
   },
   "outputs": [],
   "source": [
    "gender_classes, gender_dict = get_gender_classes(id_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "id": "BJInVqpefrVA",
    "outputId": "baaf9bbb-2edd-4a25-b600-b031a872f177"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02 (100).png</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>04 (99).png</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>05 (100).png</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>06 (99).png</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>07 (100).png</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>02 (1).png</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>06 (1).png</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>03 (100).png</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>04 (100).png</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>06 (100).png</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            image label\n",
       "0    02 (100).png     m\n",
       "1     04 (99).png     m\n",
       "2    05 (100).png     m\n",
       "3     06 (99).png     m\n",
       "4    07 (100).png     m\n",
       "..            ...   ...\n",
       "695    02 (1).png     m\n",
       "696    06 (1).png     m\n",
       "697  03 (100).png     m\n",
       "698  04 (100).png     m\n",
       "699  06 (100).png     m\n",
       "\n",
       "[700 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store the labels ('m' or 'f') for each one of the images from the training set into a dataframe\n",
    "training_labels = pd.DataFrame()\n",
    "image_names = os.listdir(cd + '/train')\n",
    "for name in image_names:\n",
    "    new_name = name.split('(')[1].replace(').png', '')\n",
    "    new_name = int(new_name)\n",
    "    tmp_class = int_to_class(new_name)\n",
    "    tmp_series = pd.Series({'image': name, 'label': gender_dict[tmp_class]}).to_frame().T\n",
    "    training_labels = pd.concat([training_labels, tmp_series])\n",
    "    \n",
    "training_labels.index = range(700)\n",
    "training_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "JLGbumqNfuPZ"
   },
   "outputs": [],
   "source": [
    "# Prepare training data and initialize kfold cross-validation\n",
    "train_data = training_labels\n",
    "Y = train_data['label']\n",
    "kfold = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "vin92S9kfxvC"
   },
   "outputs": [],
   "source": [
    "# Creating an instance of the ImageDataGenerator class\n",
    "image_data_gen = ImageDataGenerator(width_shift_range=0.1,\n",
    "                         height_shift_range=0.1,\n",
    "                         zoom_range=0.3,\n",
    "                         fill_mode='nearest',\n",
    "                         horizontal_flip = True,\n",
    "                         rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oA-HfFzLvnu-",
    "outputId": "841a3960-244c-4489-9a53-f652308f4e58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 560 validated image filenames belonging to 2 classes.\n",
      "Found 140 validated image filenames belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "18/18 [==============================] - 126s 7s/step - loss: 0.6915 - accuracy: 0.8639 - auc: 0.5045 - val_loss: 0.6888 - val_accuracy: 0.9500 - val_auc: 0.8569\n",
      "\n",
      "Epoch 00001: saving model to /content/drive/My Drive/Assignment_3/models/model_1.h5\n",
      "Epoch 2/5\n",
      "18/18 [==============================] - 9s 517ms/step - loss: 0.6887 - accuracy: 0.8968 - auc: 0.8697 - val_loss: 0.6856 - val_accuracy: 0.9500 - val_auc: 0.9568\n",
      "\n",
      "Epoch 00002: saving model to /content/drive/My Drive/Assignment_3/models/model_1.h5\n",
      "Epoch 3/5\n",
      "18/18 [==============================] - 9s 503ms/step - loss: 0.6859 - accuracy: 0.8920 - auc: 0.8912 - val_loss: 0.6824 - val_accuracy: 0.9500 - val_auc: 0.9500\n",
      "\n",
      "Epoch 00003: saving model to /content/drive/My Drive/Assignment_3/models/model_1.h5\n",
      "Epoch 4/5\n",
      "18/18 [==============================] - 9s 524ms/step - loss: 0.6828 - accuracy: 0.9047 - auc: 0.9033 - val_loss: 0.6793 - val_accuracy: 0.9500 - val_auc: 0.9561\n",
      "\n",
      "Epoch 00004: saving model to /content/drive/My Drive/Assignment_3/models/model_1.h5\n",
      "Epoch 5/5\n",
      "18/18 [==============================] - 9s 501ms/step - loss: 0.6797 - accuracy: 0.9125 - auc: 0.9130 - val_loss: 0.6762 - val_accuracy: 0.9500 - val_auc: 0.9500\n",
      "\n",
      "Epoch 00005: saving model to /content/drive/My Drive/Assignment_3/models/model_1.h5\n",
      "5/5 [==============================] - 2s 326ms/step - loss: 0.6760 - accuracy: 0.9500 - auc: 0.9500\n",
      "Found 560 validated image filenames belonging to 2 classes.\n",
      "Found 140 validated image filenames belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "18/18 [==============================] - 10s 577ms/step - loss: 0.6763 - accuracy: 0.9125 - auc: 0.9123 - val_loss: 0.6754 - val_accuracy: 0.9000 - val_auc: 0.9000\n",
      "\n",
      "Epoch 00001: saving model to /content/drive/My Drive/Assignment_3/models/model_2.h5\n",
      "Epoch 2/5\n",
      "18/18 [==============================] - 9s 497ms/step - loss: 0.6735 - accuracy: 0.9125 - auc: 0.9104 - val_loss: 0.6726 - val_accuracy: 0.9000 - val_auc: 0.9000\n",
      "\n",
      "Epoch 00002: saving model to /content/drive/My Drive/Assignment_3/models/model_2.h5\n",
      "Epoch 3/5\n",
      "18/18 [==============================] - 11s 523ms/step - loss: 0.6705 - accuracy: 0.9125 - auc: 0.9127 - val_loss: 0.6699 - val_accuracy: 0.9000 - val_auc: 0.9000\n",
      "\n",
      "Epoch 00003: saving model to /content/drive/My Drive/Assignment_3/models/model_2.h5\n",
      "Epoch 4/5\n",
      "18/18 [==============================] - 9s 527ms/step - loss: 0.6677 - accuracy: 0.9125 - auc: 0.9125 - val_loss: 0.6671 - val_accuracy: 0.9000 - val_auc: 0.9000\n",
      "\n",
      "Epoch 00004: saving model to /content/drive/My Drive/Assignment_3/models/model_2.h5\n",
      "Epoch 5/5\n",
      "18/18 [==============================] - 10s 548ms/step - loss: 0.6649 - accuracy: 0.9125 - auc: 0.9075 - val_loss: 0.6644 - val_accuracy: 0.9000 - val_auc: 0.9014\n",
      "\n",
      "Epoch 00005: saving model to /content/drive/My Drive/Assignment_3/models/model_2.h5\n",
      "5/5 [==============================] - 2s 448ms/step - loss: 0.6644 - accuracy: 0.9000 - auc: 0.9007\n",
      "Found 560 validated image filenames belonging to 2 classes.\n",
      "Found 140 validated image filenames belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "18/18 [==============================] - 10s 527ms/step - loss: 0.6632 - accuracy: 0.9000 - auc: 0.9000 - val_loss: 0.6577 - val_accuracy: 0.9500 - val_auc: 0.9500\n",
      "\n",
      "Epoch 00001: saving model to /content/drive/My Drive/Assignment_3/models/model_3.h5\n",
      "Epoch 2/5\n",
      "18/18 [==============================] - 9s 513ms/step - loss: 0.6605 - accuracy: 0.9000 - auc: 0.8998 - val_loss: 0.6547 - val_accuracy: 0.9500 - val_auc: 0.9500\n",
      "\n",
      "Epoch 00002: saving model to /content/drive/My Drive/Assignment_3/models/model_3.h5\n",
      "Epoch 3/5\n",
      "18/18 [==============================] - 11s 574ms/step - loss: 0.6579 - accuracy: 0.9000 - auc: 0.8954 - val_loss: 0.6517 - val_accuracy: 0.9500 - val_auc: 0.9486\n",
      "\n",
      "Epoch 00003: saving model to /content/drive/My Drive/Assignment_3/models/model_3.h5\n",
      "Epoch 4/5\n",
      "18/18 [==============================] - 9s 518ms/step - loss: 0.6553 - accuracy: 0.9000 - auc: 0.8989 - val_loss: 0.6488 - val_accuracy: 0.9500 - val_auc: 0.9500\n",
      "\n",
      "Epoch 00004: saving model to /content/drive/My Drive/Assignment_3/models/model_3.h5\n",
      "Epoch 5/5\n",
      "18/18 [==============================] - 10s 535ms/step - loss: 0.6527 - accuracy: 0.9000 - auc: 0.9000 - val_loss: 0.6459 - val_accuracy: 0.9500 - val_auc: 0.9504\n",
      "\n",
      "Epoch 00005: saving model to /content/drive/My Drive/Assignment_3/models/model_3.h5\n",
      "5/5 [==============================] - 2s 332ms/step - loss: 0.6459 - accuracy: 0.9500 - auc: 0.9507\n",
      "Found 560 validated image filenames belonging to 2 classes.\n",
      "Found 140 validated image filenames belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "18/18 [==============================] - 10s 576ms/step - loss: 0.6469 - accuracy: 0.9286 - auc: 0.9292 - val_loss: 0.6561 - val_accuracy: 0.8357 - val_auc: 0.8310\n",
      "\n",
      "Epoch 00001: saving model to /content/drive/My Drive/Assignment_3/models/model_4.h5\n",
      "Epoch 2/5\n",
      "18/18 [==============================] - 13s 563ms/step - loss: 0.6440 - accuracy: 0.9286 - auc: 0.9281 - val_loss: 0.6539 - val_accuracy: 0.8357 - val_auc: 0.8357\n",
      "\n",
      "Epoch 00002: saving model to /content/drive/My Drive/Assignment_3/models/model_4.h5\n",
      "Epoch 3/5\n",
      "18/18 [==============================] - 9s 513ms/step - loss: 0.6411 - accuracy: 0.9286 - auc: 0.9291 - val_loss: 0.6517 - val_accuracy: 0.8357 - val_auc: 0.8339\n",
      "\n",
      "Epoch 00003: saving model to /content/drive/My Drive/Assignment_3/models/model_4.h5\n",
      "Epoch 4/5\n",
      "18/18 [==============================] - 9s 526ms/step - loss: 0.6383 - accuracy: 0.9286 - auc: 0.9330 - val_loss: 0.6496 - val_accuracy: 0.8357 - val_auc: 0.8334\n",
      "\n",
      "Epoch 00004: saving model to /content/drive/My Drive/Assignment_3/models/model_4.h5\n",
      "Epoch 5/5\n",
      "18/18 [==============================] - 9s 493ms/step - loss: 0.6355 - accuracy: 0.9286 - auc: 0.9284 - val_loss: 0.6474 - val_accuracy: 0.8357 - val_auc: 0.8357\n",
      "\n",
      "Epoch 00005: saving model to /content/drive/My Drive/Assignment_3/models/model_4.h5\n",
      "5/5 [==============================] - 2s 322ms/step - loss: 0.6473 - accuracy: 0.8357 - auc: 0.8357\n",
      "Found 560 validated image filenames belonging to 2 classes.\n",
      "Found 140 validated image filenames belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "18/18 [==============================] - 10s 568ms/step - loss: 0.6356 - accuracy: 0.9089 - auc: 0.9081 - val_loss: 0.6335 - val_accuracy: 0.9143 - val_auc: 0.9063\n",
      "\n",
      "Epoch 00001: saving model to /content/drive/My Drive/Assignment_3/models/model_5.h5\n",
      "Epoch 2/5\n",
      "18/18 [==============================] - 10s 538ms/step - loss: 0.6330 - accuracy: 0.9089 - auc: 0.9057 - val_loss: 0.6308 - val_accuracy: 0.9143 - val_auc: 0.9137\n",
      "\n",
      "Epoch 00002: saving model to /content/drive/My Drive/Assignment_3/models/model_5.h5\n",
      "Epoch 3/5\n",
      "18/18 [==============================] - 9s 512ms/step - loss: 0.6305 - accuracy: 0.9089 - auc: 0.9089 - val_loss: 0.6283 - val_accuracy: 0.9143 - val_auc: 0.9143\n",
      "\n",
      "Epoch 00003: saving model to /content/drive/My Drive/Assignment_3/models/model_5.h5\n",
      "Epoch 4/5\n",
      "18/18 [==============================] - 13s 546ms/step - loss: 0.6280 - accuracy: 0.9089 - auc: 0.9106 - val_loss: 0.6258 - val_accuracy: 0.9143 - val_auc: 0.9069\n",
      "\n",
      "Epoch 00004: saving model to /content/drive/My Drive/Assignment_3/models/model_5.h5\n",
      "Epoch 5/5\n",
      "18/18 [==============================] - 9s 514ms/step - loss: 0.6255 - accuracy: 0.9089 - auc: 0.9081 - val_loss: 0.6233 - val_accuracy: 0.9143 - val_auc: 0.9143\n",
      "\n",
      "Epoch 00005: saving model to /content/drive/My Drive/Assignment_3/models/model_5.h5\n",
      "5/5 [==============================] - 2s 331ms/step - loss: 0.6233 - accuracy: 0.9143 - auc: 0.9137\n"
     ]
    }
   ],
   "source": [
    "# 5-fold cross-validation of our model\n",
    "validation_accuracy, validation_loss, validation_auc = [], [], []\n",
    "\n",
    "fold_var = 1\n",
    "save_dir = cd + '/models/'\n",
    "\n",
    "for train_index, val_index in kfold.split(np.zeros(700), Y):\n",
    "    training_data = train_data.iloc[train_index]\n",
    "    validation_data = train_data.iloc[val_index]\n",
    "    train_data_generator = image_data_gen.flow_from_dataframe(training_data, directory = cd + '/train',\n",
    "                                                              x_col = \"image\", y_col = \"label\",\n",
    "                                                              class_mode = \"categorical\", target_size=(224, 224),\n",
    "                                                              shuffle = True)\n",
    "    valid_data_generator  = image_data_gen.flow_from_dataframe(validation_data, directory = cd + '/train',\n",
    "                                                               x_col = \"image\", y_col = \"label\",\n",
    "                                                               class_mode = \"categorical\", target_size=(224, 224),\n",
    "                                                               shuffle = True)\n",
    "    current_model = model\n",
    "  \n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(save_dir + get_model_name(fold_var), \n",
    "                                                 monitor=['val_accuracy', 'val_auc'], verbose=2, \n",
    "                                                 save_best_only=False, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    current_model.fit(train_data_generator,\n",
    "                      epochs=5,\n",
    "                      callbacks=callbacks_list,\n",
    "                      validation_data=valid_data_generator)\n",
    "  \n",
    "    current_model.load_weights(cd + \"/models/model_\" + str(fold_var) + \".h5\")\n",
    "\n",
    "    results = current_model.evaluate(valid_data_generator)\n",
    "    results = dict(zip(current_model.metrics_names, results))\n",
    "\n",
    "    validation_accuracy.append(results['accuracy'])\n",
    "    validation_loss.append(results['loss'])\n",
    "    validation_auc.append(results['auc'])\n",
    "\n",
    "    K.clear_session()\n",
    "    \n",
    "    fold_var += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vqiWlLn1dLKj",
    "outputId": "7c75be7e-92d7-45e0-a5b8-1423d5c4595b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6760216951370239,\n",
       " 0.6643847823143005,\n",
       " 0.6459063291549683,\n",
       " 0.6473250389099121,\n",
       " 0.6232863664627075]"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z148psK5wD-k",
    "outputId": "12062ecc-c915-4106-a579-66dd02cfdca0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.949999988079071,\n",
       " 0.8999999761581421,\n",
       " 0.949999988079071,\n",
       " 0.8357142806053162,\n",
       " 0.9142857193946838]"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1fifNstAdIr8",
    "outputId": "09f02477-68a8-4e80-f551-49296d8fb219"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9500000476837158,\n",
       " 0.9007142782211304,\n",
       " 0.9507142901420593,\n",
       " 0.8357143402099609,\n",
       " 0.9136734008789062]"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "id": "HuSQYH0qdPXN",
    "outputId": "a7740d50-da00-4894-ab66-7e30da950f16"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>08 (100).png</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10 (100).png</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>08 (99).png</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>09 (100).png</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10 (99).png</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>09 (4).png</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>08 (1).png</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>09 (3).png</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>09 (2).png</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>09 (1).png</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            image label\n",
       "0    08 (100).png     m\n",
       "1    10 (100).png     m\n",
       "2     08 (99).png     m\n",
       "3    09 (100).png     m\n",
       "4     10 (99).png     m\n",
       "..            ...   ...\n",
       "295    09 (4).png     m\n",
       "296    08 (1).png     m\n",
       "297    09 (3).png     m\n",
       "298    09 (2).png     m\n",
       "299    09 (1).png     m\n",
       "\n",
       "[300 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store the labels ('m' or 'f') for each one of the images from the test set into a dataframe\n",
    "test_labels = pd.DataFrame()\n",
    "image_names = os.listdir(cd + '/test')\n",
    "for name in image_names:\n",
    "    new_name = name.split('(')[1].replace(').png', '')\n",
    "    new_name = int(new_name)\n",
    "    tmp_class = int_to_class(new_name)\n",
    "    tmp_series = pd.Series({'image': name, 'label': gender_dict[tmp_class]}).to_frame().T\n",
    "    test_labels = pd.concat([test_labels, tmp_series])\n",
    "    \n",
    "test_labels.index = range(300)\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "zTn5JGPEkO0Z"
   },
   "outputs": [],
   "source": [
    "test_data = test_labels\n",
    "Y = test_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WcO5gB-GkBBL",
    "outputId": "cde22803-1dd5-4941-f4a5-6876c86487c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 300 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data_generator = image_data_gen.flow_from_dataframe(test_data, directory = cd + '/test',\n",
    "                                                         x_col = \"image\", y_col = \"label\",\n",
    "                                                         class_mode = \"categorical\", target_size=(224, 224),\n",
    "                                                         shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "zEhbT8Eql2qY"
   },
   "outputs": [],
   "source": [
    "model.load_weights(cd + \"/models/model_\" + str(5) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "BMiyqrAvkz3y"
   },
   "outputs": [],
   "source": [
    "Y_predicted = model.predict(test_data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "gmzbA7KvpSuY"
   },
   "outputs": [],
   "source": [
    "Y_predicted = np.argmax(Y_predicted, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "H110MHf7tWxh"
   },
   "outputs": [],
   "source": [
    "Y_predicted = pd.Series(Y_predicted)\n",
    "Y_predicted[Y_predicted == 'm'] = 1\n",
    "Y_predicted[Y_predicted == 'f'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t0Lw22JCrcbo",
    "outputId": "add85bf6-78ac-4e7e-dc67-ad673813b901"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "295    1\n",
       "296    1\n",
       "297    1\n",
       "298    1\n",
       "299    1\n",
       "Name: label, Length: 300, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[Y == 'm'] = 1\n",
    "Y[Y == 'f'] = 0\n",
    "Y = Y.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tyjMlm8oKxRT",
    "outputId": "54dfe1fe-2087-4c7e-f4f5-36f2accac536"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy = (Y == Y_predicted).sum() / len(Y)\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "SG9Ye4gZoN91",
    "outputId": "b70fc7a3-1c6f-4692-ad2b-3b74d96dc7bd"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-4e08cee921e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfpr_keras\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr_keras\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds_keras\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_predicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \"\"\"\n\u001b[1;32m    770\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0;32m--> 771\u001b[0;31m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m     \u001b[0;31m# Attempt to drop thresholds corresponding to points in between and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    534\u001b[0m     if not (y_type == \"binary\" or\n\u001b[1;32m    535\u001b[0m             (y_type == \"multiclass\" and pos_label is not None)):\n\u001b[0;32m--> 536\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: unknown format is not supported"
     ]
    }
   ],
   "source": [
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(Y.values, Y_predicted, pos_label=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZgJzI2L85Qo3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ear_recognition.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
